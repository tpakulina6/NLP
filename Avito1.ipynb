{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = dataset.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_ = test_dataset.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489517, 4) (489517,)\n",
      "(318186, 4) (318186,) (171331, 4) (171331,)\n"
     ]
    }
   ],
   "source": [
    "target = dataset_[:,4]\n",
    "target=target.astype('int')\n",
    "data = dataset_[:,:-1]\n",
    "data[:,0] = data[:,0].astype('int')\n",
    "data[:,3] = data[:,3].astype('int')\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35, random_state = 42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_pd = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем текст и переведем все символы в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i,1] = preprocess(X_train[i,1])\n",
    "    X_train[i,2] = preprocess(X_train[i,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test)):\n",
    "    X_test[i,1] = preprocess(X_test[i,1])\n",
    "    X_test[i,2] = preprocess(X_test[i,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(X_test)):\n",
    "    test_dataset_[i,1] = preprocess(test_dataset_[i,1])\n",
    "    test_dataset_[i,2] = preprocess(test_dataset_[i,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим матрицы TF-IDF для двух текстовых столбцов поотдельности и потом соединим их в одну матрицу. К ней добавим нормализованный столбец цен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "title_train_tfidf = tfidf_vectorizer.fit_transform(X_train[:,1])\n",
    "title_test_tfidf = tfidf_vectorizer.transform(X_test[:,1])\n",
    "title_dataset = tfidf_vectorizer.transform(test_dataset_[:,1])\n",
    "\n",
    "description_train_tfidf = tfidf_vectorizer.fit_transform(X_train[:,2])\n",
    "description_test_tfidf = tfidf_vectorizer.transform(X_test[:,2])\n",
    "description_dataset = tfidf_vectorizer.transform(test_dataset_[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_1 = np.array(X_train[:,3].astype(float)).reshape(-1,1)\n",
    "test_1 = np.array(X_test[:,3].astype(float)).reshape(-1,1)\n",
    "dataset_1 = np.array(test_dataset_[:,3].astype(float)).reshape(-1,1)\n",
    "\n",
    "train_ = preprocessing.normalize(train_1, axis = 0)\n",
    "test_ = preprocessing.normalize(test_1, axis = 0)\n",
    "dataset_ = preprocessing.normalize(dataset_1, axis = 0)\n",
    "\n",
    "#X_train_counts = sp.sparse.hstack(( title_train_counts, description_train_counts,csr_matrix(train_) ))\n",
    "#X_test_counts = sp.sparse.hstack(( title_test_counts, description_test_counts,csr_matrix(test_) ))\n",
    "\n",
    "X_train_tfidf = sp.sparse.hstack(( title_train_tfidf, description_train_tfidf,csr_matrix(train_) ))\n",
    "X_test_tfidf = sp.sparse.hstack(( title_test_tfidf, description_test_tfidf,csr_matrix(test_) ))\n",
    "test_dataset_tdidf = sp.sparse.hstack(( title_dataset, description_dataset,csr_matrix(dataset_) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим на получившейся матрице обычную логрегрессию и посчитаем accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=7.0, solver = 'saga', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted_counts = lr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8805995412388885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_predicted_counts)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всякий случай посчитаем точность разными способами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.881, precision = 0.880, recall = 0.881, f1 = 0.880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_predicted_counts, pos_label=None,average='weighted')             \n",
    "recall = recall_score(y_test, y_predicted_counts, pos_label=None, average='weighted')\n",
    "f1 = f1_score(y_test, y_predicted_counts, pos_label=None, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_predicted_counts)\n",
    "\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем категории на тестовых данных и запишем их в файл result1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr.predict(test_dataset_tdidf)\n",
    "y = np.array(y).reshape(-1,1)\n",
    "id_ = np.array(test_dataset_[:,0]).reshape(-1,1)\n",
    "\n",
    "result1 = np.hstack((id_,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('result1.csv', sep='\\t')\n",
    "import csv\n",
    " \n",
    "FILENAME = \"result1.csv\"\n",
    "\n",
    "with open(FILENAME, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(result1)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с иерархией категорий, для этого создадим матрицу, в которой каждый столбец отвечает одному уровню иерархии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = pd.read_csv('category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = category.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = []\n",
    "\n",
    "for s in category[:,1]:\n",
    "    if s.find('Бытовая электроника') == 0:\n",
    "        y1.append(0)\n",
    "    elif  s.find('Для дома и дачи') == 0:\n",
    "        y1.append(1)\n",
    "    elif  s.find('Личные вещи') == 0:\n",
    "        y1.append(2)\n",
    "    elif  s.find('Хобби и отдых') == 0:\n",
    "        y1.append(3)\n",
    "    else :\n",
    "        y1.append(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = []\n",
    "\n",
    "for s in category[:,1]:\n",
    "    if s.find('Бытовая электроника|Телефоны') == 0:\n",
    "        y2.append(0)\n",
    "    elif  s.find('Бытовая электроника|Игры, приставки и программы') == 0:\n",
    "        y2.append(1)\n",
    "    elif  s.find('Бытовая электроника|Аудио и видео') == 0:\n",
    "        y2.append(2)\n",
    "    elif  s.find('Бытовая электроника|Товары для компьютера') == 0:\n",
    "        y2.append(3)\n",
    "    elif  s.find('Для дома и дачи|Ремонт и строительство') == 0:\n",
    "        y2.append(4)\n",
    "    elif  s.find('Для дома и дачи|Мебель и интерьер') == 0:\n",
    "        y2.append(5)\n",
    "    elif  s.find('Для дома и дачи|Бытовая техника') == 0:\n",
    "        y2.append(6)\n",
    "    elif  s.find('Личные вещи|Товары для детей и игрушки') == 0:\n",
    "        y2.append(7)\n",
    "    elif  s.find('Личные вещи|Одежда, обувь, аксессуары') == 0:\n",
    "        y2.append(8)\n",
    "    elif  s.find('Личные вещи|Часы и украшения') == 0:\n",
    "        y2.append(9)\n",
    "    elif  s.find('Хобби и отдых|Спорт и отдых') == 0:\n",
    "        y2.append(10)\n",
    "    elif  s.find('Хобби и отдых|Книги и журналы') == 0:\n",
    "        y2.append(11)\n",
    "    elif  s.find('Хобби и отдых|Коллекционирование') == 0:\n",
    "        y2.append(12)\n",
    "    elif  s.find('Хобби и отдых|Музыкальные инструменты') == 0:\n",
    "        y2.append(13)\n",
    "    else :\n",
    "        y2.append(-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = []\n",
    "\n",
    "for s in category[:,1]:\n",
    "    if s.find('Личные вещи|Одежда, обувь, аксессуары|Женская одежда') == 0:\n",
    "        y3.append(0)\n",
    "    else :\n",
    "        y3.append(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(y1).reshape(-1,1)\n",
    "y2 = np.array(y2).reshape(-1,1)\n",
    "y3 = np.array(y3).reshape(-1,1)\n",
    "\n",
    "transl = np.hstack((y1,y2,y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность предсказания на 1 (самом низком) уровне иерархии категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9622952063549504\n"
     ]
    }
   ],
   "source": [
    "n = len(y_test)\n",
    "y_1  = np.zeros(n)\n",
    "y_pred_1 = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    if transl[y_test[i],0] != -1:\n",
    "        y_1[i] = transl[y_test[i],0]\n",
    "        y_pred_1[i] = transl[y_predicted_counts[i],0]\n",
    "    else:\n",
    "        y_1[i] = y_test[i]\n",
    "        y_pred_1[i] = y_predicted_counts[i]\n",
    "print(accuracy_score(y_1, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На втором уровне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9427307375781382\n"
     ]
    }
   ],
   "source": [
    "n = len(y_test)\n",
    "y_2  = np.zeros(n)\n",
    "y_pred_2 = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    if transl[y_test[i],1] != -1:\n",
    "        y_2[i] = transl[y_test[i],1]\n",
    "        y_pred_2[i] = transl[y_predicted_counts[i],1]\n",
    "    else:\n",
    "        y_2[i] = y_test[i]\n",
    "        y_pred_2[i] = y_predicted_counts[i]\n",
    "    \n",
    "print(accuracy_score(y_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На третьем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8848719729646124\n"
     ]
    }
   ],
   "source": [
    "n = len(y_test)\n",
    "y_3  = np.zeros(n)\n",
    "y_pred_3 = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    if transl[y_test[i],2] != -1:\n",
    "        y_3[i] = transl[y_test[i],2]\n",
    "        y_pred_3[i] = transl[y_predicted_counts[i],2]\n",
    "    else:\n",
    "        y_3[i] = y_test[i]\n",
    "        y_pred_3[i] = y_predicted_counts[i]\n",
    "    \n",
    "print(accuracy_score(y_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем Word2Vec. Для этого сначала подготовим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_train = dataset.apply(lambda x: x['title'] + ' . ' + x['description'], 1)\n",
    "all_data_test = test_dataset.apply(lambda x: x['title'] + ' . ' + x['description'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc = all_data_train.apply(lambda x: preprocess(x), 1)\n",
    "test_proc = all_data_train.apply(lambda x: preprocess(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tokens'] = train_proc.apply(lambda x: x.split(), 1)\n",
    "test_dataset['tokens'] = test_proc.apply(lambda x: x.split(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for s in train_proc:\n",
    "    sentences = nltk.sent_tokenize(s)\n",
    "    all_words += [sent.split() for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in test_proc:\n",
    "    sentences = nltk.sent_tokenize(s)\n",
    "    all_words += [sent.split() for sent in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, size = 200, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, data, generate_missing=False):\n",
    "    embeddings = data['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "embeddings = get_word2vec_embeddings(word2vec, dataset)\n",
    "\n",
    "price = np.array(dataset['price'].astype(float)).reshape(-1,1)\n",
    "price_ = preprocessing.normalize(price, axis = 0)\n",
    "embeddings_ = np.hstack((embeddings, price_))\n",
    "embeddings_ = np.array(embeddings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391613, 201) (391613,) (97904, 201) (97904,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings_, target, \n",
    "                                                                                        test_size=0.2, random_state=40)\n",
    "print(X_train_word2vec.shape, y_train_word2vec.shape, X_test_word2vec.shape, y_test_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = get_word2vec_embeddings(word2vec, test_dataset)\n",
    "\n",
    "price_test = np.array(test_dataset['price'].astype(float)).reshape(-1,1)\n",
    "price_test_ = preprocessing.normalize(price_test, axis = 0)\n",
    "\n",
    "embeddings_test_ = np.hstack((embeddings_test, price_test_))\n",
    "embeddings_test_ = np.array(embeddings_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_w2v = LogisticRegression(C=30.0, solver = 'saga', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "lr_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "\n",
    "y_pred_w2v = lr_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.824, precision = 0.819, recall = 0.824, f1 = 0.821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "precision = precision_score(y_test_word2vec, y_pred_w2v, pos_label=None,average='weighted')             \n",
    "recall = recall_score(y_test_word2vec, y_pred_w2v, pos_label=None, average='weighted')\n",
    "f1 = f1_score(y_test_word2vec, y_pred_w2v, pos_label=None, average='weighted')\n",
    "accuracy = accuracy_score(y_test_word2vec, y_pred_w2v)\n",
    "#print(classification_report(y_test_word2vec, y_pred_w2v))\n",
    "\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_w2v.predict(embeddings_test_)\n",
    "y = np.array(y).reshape(-1,1)\n",
    "id_ = np.array(test_dataset_[:,0]).reshape(-1,1)\n",
    "\n",
    "result2 = np.hstack((id_,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    " \n",
    "FILENAME = \"result2.csv\"\n",
    "\n",
    "with open(FILENAME, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну так как тут точность меньше 88, то по уровням я решила не делать . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
